{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ddb93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/misenta/CellViT-plus-plus/cellvit/models/utils/hibou_utils.py:54: UserWarning: xFormers is disabled\n",
      "  warnings.warn(\"xFormers is disabled\")\n",
      "/scratch/users/misenta/CellViT-plus-plus/cellvit/models/utils/hibou_utils.py:60: UserWarning: xFormers is not available\n",
      "  warnings.warn(\"xFormers is not available\")\n",
      "/opt/conda/envs/tissuevit/lib/python3.10/site-packages/cupy/_environment.py:596: UserWarning: \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "\n",
      "    cupy, cupy-cuda12x\n",
      "\n",
      "  Follow these steps to resolve this issue:\n",
      "\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "\n",
      "         $ pip uninstall <package_name>\n",
      "\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "\n",
      "         $ conda uninstall cupy\n",
      "\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "\n",
      "         https://docs.cupy.dev/en/stable/install.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  warnings.warn(f'''\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import types\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Disable xformers to avoid attention-kernel issues across envs\n",
    "os.environ.setdefault('XFORMERS_DISABLED', '1')\n",
    "\n",
    "# Stub flash_attn to avoid binary import on environments without it\n",
    "if 'flash_attn.flash_attn_interface' not in sys.modules:\n",
    "    flash_attn_interface = types.ModuleType('flash_attn.flash_attn_interface')\n",
    "    def flash_attn_func(*args, **kwargs):\n",
    "        raise ImportError('flash_attn disabled for this smoke test')\n",
    "    flash_attn_interface.flash_attn_func = flash_attn_func\n",
    "    flash_attn = types.ModuleType('flash_attn')\n",
    "    flash_attn.flash_attn_interface = flash_attn_interface\n",
    "    sys.modules['flash_attn'] = flash_attn\n",
    "    sys.modules['flash_attn.flash_attn_interface'] = flash_attn_interface\n",
    "\n",
    "import cellvit.models.cell_segmentation.backbones_mmvirtues as backbones_mmvirtues\n",
    "importlib.reload(backbones_mmvirtues)\n",
    "import cellvit.models.cell_segmentation.cellvit_mmvirtues as cellvit_mmvirtues\n",
    "importlib.reload(cellvit_mmvirtues)\n",
    "from cellvit.models.cell_segmentation.cellvit_mmvirtues import CellViTMMVirtues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9af74e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmvirtues_root: /scratch/mmvirtues_orion_dataset/virtues_example\n",
      "weights_dir: /scratch/mmvirtues_orion_dataset/virtues_example/mmvirtues_weights\n"
     ]
    }
   ],
   "source": [
    "# --- mmVIRTUES paths ---\n",
    "default_root = Path('/scratch/mmvirtues_orion_dataset/virtues_example')\n",
    "mmvirtues_root = Path(os.environ.get('MMVIRTUES_ROOT', str(default_root))).resolve()\n",
    "weights_dir = Path(os.environ.get('MMVIRTUES_WEIGHTS', str(mmvirtues_root / 'mmvirtues_weights'))).resolve()\n",
    "\n",
    "print('mmvirtues_root:', mmvirtues_root)\n",
    "print('weights_dir:', weights_dir)\n",
    "\n",
    "assert mmvirtues_root.exists(), f'MMVIRTUES_ROOT not found: {mmvirtues_root}'\n",
    "assert (mmvirtues_root / 'datasets_loading').exists(), f'Expected datasets_loading/ under {mmvirtues_root}'\n",
    "assert (mmvirtues_root / 'esm2_t30_150M_UR50D').exists(), f'Expected esm2_t30_150M_UR50D/ under {mmvirtues_root}'\n",
    "assert (weights_dir / 'config.yaml').exists(), f'Missing config.yaml in {weights_dir}'\n",
    "assert (weights_dir / 'teacher_checkpoint.pth').exists(), f'Missing teacher_checkpoint.pth in {weights_dir}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b899d948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (2, 3, 256, 256) torch.float32 range: 6.556510925292969e-07 0.9999989867210388\n",
      "{'instance_map': ((2, 256, 256), torch.int64), 'nuclei_type_map': ((2, 256, 256), torch.int64), 'nuclei_binary_map': ((2, 256, 256), torch.int64), 'hv_map': ((2, 2, 256, 256), torch.float32)}\n"
     ]
    }
   ],
   "source": [
    "# --- Synthetic PanNuke-like batch (dims + mask keys) ---\n",
    "B, H, W = 2, 256, 256\n",
    "x = torch.rand(B, 3, H, W, dtype=torch.float32)\n",
    "\n",
    "yy, xx = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')\n",
    "instance_map = torch.zeros((B, H, W), dtype=torch.int64)\n",
    "nuclei_type_map = torch.zeros((B, H, W), dtype=torch.int64)\n",
    "nuclei_binary_map = torch.zeros((B, H, W), dtype=torch.int64)\n",
    "hv_map = torch.zeros((B, 2, H, W), dtype=torch.float32)\n",
    "\n",
    "# A few fake nuclei (simple disks) with types in {1..5}\n",
    "for b in range(B):\n",
    "    for inst_id, (cy, cx, r, t) in enumerate([(64, 64, 18, 1), (160, 120, 24, 3), (120, 200, 16, 5)], start=1):\n",
    "        m = (yy - cy) ** 2 + (xx - cx) ** 2 <= r ** 2\n",
    "        instance_map[b][m] = inst_id\n",
    "        nuclei_type_map[b][m] = t\n",
    "        nuclei_binary_map[b][m] = 1\n",
    "\n",
    "masks = {\n",
    "    'instance_map': instance_map,\n",
    "    'nuclei_type_map': nuclei_type_map,\n",
    "    'nuclei_binary_map': nuclei_binary_map,\n",
    "    'hv_map': hv_map,\n",
    "}\n",
    "\n",
    "print('x:', tuple(x.shape), x.dtype, 'range:', float(x.min()), float(x.max()))\n",
    "print({k: (tuple(v.shape), v.dtype) for k, v in masks.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de070790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mmvirtues_orion_dataset/virtues_example/modules/mmvirtues/vit_layers/swiglu_ffn.py:45: UserWarning: xFormers is disabled (SwiGLU)\n",
      "  warnings.warn(\"xFormers is disabled (SwiGLU)\")\n",
      "/scratch/mmvirtues_orion_dataset/virtues_example/modules/mmvirtues/vit_layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/scratch/mmvirtues_orion_dataset/virtues_example/modules/mmvirtues/vit_layers/attention.py:29: UserWarning: xFormers is disabled (Attention)\n",
      "  warnings.warn(\"xFormers is disabled (Attention)\")\n",
      "/scratch/mmvirtues_orion_dataset/virtues_example/modules/mmvirtues/vit_layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/scratch/mmvirtues_orion_dataset/virtues_example/modules/mmvirtues/vit_layers/block.py:36: UserWarning: xFormers is disabled (Block)\n",
      "  warnings.warn(\"xFormers is disabled (Block)\")\n",
      "/scratch/mmvirtues_orion_dataset/virtues_example/modules/mmvirtues/vit_layers/block.py:41: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "\u001b[32m2025-12-16 16:32:31.572\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodules.mmvirtues.layers\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mUsing xformers for FlexDualVirTues\u001b[0m\n",
      "\u001b[32m2025-12-16 16:32:31.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodules.mmvirtues.flex_dual_mmvirtues\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mUsing protein embedding: esm with shape torch.Size([213, 640])\u001b[0m\n",
      "\u001b[32m2025-12-16 16:32:31.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodules.mmvirtues.flex_dual_mmvirtues\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mUsing protein fusion type: add\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['dino_head.mlp.0.weight', 'dino_head.mlp.0.bias', 'dino_head.mlp.2.weight', 'dino_head.mlp.2.bias', 'dino_head.mlp.4.weight', 'dino_head.mlp.4.bias', 'dino_head.last_layer.weight_g', 'dino_head.last_layer.weight_v'])\n",
      "{'tissue_types': (2, 19), 'nuclei_binary_map': (2, 2, 256, 256), 'hv_map': (2, 2, 256, 256), 'nuclei_type_map': (2, 6, 256, 256)}\n",
      "marker_embeddings_dir: /scratch/mmvirtues_orion_dataset/virtues_example/marker_embeddings_symlink\n"
     ]
    }
   ],
   "source": [
    "# --- Build model + forward ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)\n",
    "\n",
    "model = CellViTMMVirtues(\n",
    "    mmvirtues_weights_path=weights_dir,\n",
    "    mmvirtues_root=mmvirtues_root,\n",
    "    num_nuclei_classes=6,  # PanNuke: background + 5 types\n",
    "    num_tissue_classes=19,\n",
    "    regression_loss=False,\n",
    ")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "x_dev = x.to(device)\n",
    "with torch.no_grad():\n",
    "    out = model(x_dev)\n",
    "\n",
    "print({k: (tuple(v.shape) if torch.is_tensor(v) else type(v)) for k, v in out.items()})\n",
    "print('marker_embeddings_dir:', model.encoder.marker_embeddings_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a3efe0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_total: 2.7761454582214355\n",
      "loss_nb: 0.7834705114364624 loss_nt: 1.7265998125076294 loss_hv: 0.26607513427734375\n"
     ]
    }
   ],
   "source": [
    "# --- Optional: 1-step backward (freeze mmVIRTUES encoder to keep it light) ---\n",
    "for p in model.encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-4)\n",
    "\n",
    "x_train = x.to(device)\n",
    "masks_train = {k: v.to(device) for k, v in masks.items()}\n",
    "\n",
    "out = model(x_train)\n",
    "\n",
    "nb_logits = out['nuclei_binary_map']\n",
    "nb_target = masks_train['nuclei_binary_map'].long()\n",
    "nt_logits = out['nuclei_type_map']\n",
    "nt_target = masks_train['nuclei_type_map'].long()\n",
    "hv_pred = out['hv_map']\n",
    "hv_target = masks_train['hv_map'].float()\n",
    "\n",
    "H, W = nb_target.shape[-2:]\n",
    "if nb_logits.shape[-2:] != (H, W):\n",
    "    nb_logits = F.interpolate(nb_logits, size=(H, W), mode='bilinear', align_corners=False)\n",
    "if nt_logits.shape[-2:] != (H, W):\n",
    "    nt_logits = F.interpolate(nt_logits, size=(H, W), mode='bilinear', align_corners=False)\n",
    "if hv_pred.shape[-2:] != (H, W):\n",
    "    hv_pred = F.interpolate(hv_pred, size=(H, W), mode='bilinear', align_corners=False)\n",
    "\n",
    "loss_nb = F.cross_entropy(nb_logits, nb_target.clamp(0, 1))\n",
    "loss_nt = F.cross_entropy(nt_logits, nt_target.clamp(0, 5))\n",
    "loss_hv = F.l1_loss(hv_pred, hv_target)\n",
    "loss = loss_nb + loss_nt + loss_hv\n",
    "\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print('loss_total:', float(loss))\n",
    "print('loss_nb:', float(loss_nb), 'loss_nt:', float(loss_nt), 'loss_hv:', float(loss_hv))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tissuevit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
